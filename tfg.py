# -*- coding: utf-8 -*-
"""TFG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XUioYqm_vfUQZPIDJo2Lm3dHbqt8CZU1

#Installs
"""

!pip install accelerate>=0.20.1
!pip install transformers -U
!pip install transformers[torch]

"""Montem google drive per tal de poder guardar i descarregar arxius. Els arxius estan adjunt al git del projecte a la carpeta TFG. Per tal de que funcioni correctament, cal baixar la carpeta i adjuntar-la al drive personal."""

from google.colab import drive
drive.mount('/content/gdrive')

"""# Pre-processing del dataset

S'adjunten les funcions de preprocessament del conjunt de dades. Cal modificar el conunt d'entrenament per al fine tunning depenent de la mida que volguem utilitzar (canviar la ruta a la carpeta) i la part del conjunt de test amb la que estiguem experimentant.
"""

# pre-processat amb marques

def nerc_full_with_marks(datadir, outfile) :

    # open file to write results, this makes overwrite
    outf = open(outfile, 'w')

    # process each file in input directory
    for f in listdir(datadir) :

        # parse XML file, obtaining a DOM tree
        tree = parse(datadir+"/"+f)

        # process each sentence in the file
        sentences = tree.getElementsByTagName("sentence")
        for s in sentences :
            # sid = s.attributes["id"].value   # get sentence id

            stext = s.attributes["text"].value   # get sentence text

            # Elimina los caracteres "&#xa;" y "&#xd;" del texto
            stext = stext.replace("&#xa;", "").replace("&#xd;", "").replace("\n", "").replace("\r", "")

            print ("Sentence: ", stext, file=outf)

            entities = s.getElementsByTagName("entity")

            drugs = {}

            # print sentence entities in format requested for evaluation
            for e in entities :
                etype = e.attributes["type"].value
                etext = e.attributes["text"].value
                drugs[etext] = etype

            # replace drug entities in the sentence text
            for drug, etype in drugs.items():
                stext = stext.replace(drug, f"<{etype}>{drug}</{etype}>")



            print ("Response: ", stext, file=outf)

              # Desired format:
              # Sentence: Repeated oral administration of coumaphos in sheep: interactions of coumaphos with bishydroxycoumarin, trichlorfon, and phenobarbital sodium.
              # Response: Repeated oral administration of <drug_n>coumaphos</drug_n> in sheep: interactions of <drug_n>coumaphos</drug_n> with <drug>bishydroxycoumarin</drug>, <drug_n>trichlorfon</drug_n>, and <drug_n>phenobarbital sodium</drug_n>.

    outf.close()

def nerc_sentences(datadir, outfile) :

    # open file to write results, this makes overwrite
    outf = open(outfile, 'w')

    # process each file in input directory
    for f in listdir(datadir) :
        # parse XML file, obtaining a DOM tree
        tree = parse(datadir+"/"+f)
        # process each sentence in the file
        sentences = tree.getElementsByTagName("sentence")
        for s in sentences :
            # sid = s.attributes["id"].value   # get sentence id
            stext = s.attributes["text"].value

            # Elimina los caracteres "&#xa;" y "&#xd;" del texto
            stext = stext.replace("&#xa;", "").replace("&#xd;", "").replace("\n", "").replace("\r", "")

            print ("Sentence: ", stext, file=outf)

              # Desired format:
              # Text: In order to approximate the steady state level, serum digoxin levels should be drawn either before or at least six hours following the administration of an oral tablet.


    outf.close()

def pre_processing_FT():
  # train dataset used during the fine-tuning process to train the model (BOTH MODELS)
  datadir = "/content/gdrive/MyDrive/TFG/data/train100" #modificar depenent de la quantitat d'exemples d'entrenament que volguem proporcionar
  outfile = "/content/train_dataset.txt"
  nerc_full_with_marks(datadir, outfile)

  # devel dataset used during the fine-tuning process to use for hyperparameter tuning after train (BOTH MODELS)
  datadir = "/content/gdrive/MyDrive/TFG/data/devel"
  outfile = "/content/devel_dataset.txt"
  nerc_sentences(datadir, outfile)

# Nomes per el devel, sense fine tunning i per tant nomes els tests
def pre_processing_without_fine_tunning():

  # complete test (ZERO-SHOT LEARNING & FINE TUNNING), with all the possible predictions
  datadir = "/content/gdrive/MyDrive/TFG/data/test_p2"  #modificar depenent de la part que estem processant
  outfile = "/content/ZS_test_dataset.txt"
  nerc_sentences(datadir, outfile)

  # test few examples dataset (FEW-SHOT LEARNING), with text and predictions
  datadir = "/content/gdrive/MyDrive/TFG/data/test_FS"
  outfile = "/content/FS_test_dataset.txt"
  nerc_full_with_marks(datadir, outfile)

  # text only sentences (FEW-SHOT LEARNING), test dataset without the few examples from above
  datadir = "/content/gdrive/MyDrive/TFG/data/count_test"
  outfile = "/content/test.txt"
  nerc_sentences(datadir, outfile)

  # Complete test (for avaluation), all the test with the validated results for the avaluation of all the methods
  datadir = "/content/gdrive/MyDrive/TFG/data/test_p2" #modificar depenent de la part que estem processant
  outfile = "/content/full_test_dataset.txt"
  nerc_full_with_marks(datadir, outfile)

"""#Models

## Utils
"""

def number_of_lines(path):
  with open(path, "r", encoding="utf-8") as file:
      df = file.read().splitlines()

  print("Number of lines:", len(df))

# Codi per guardar els resultats de les prediccions
def save_results(generated_text, outfile):

    # open file to write results, this makes overwrite
    outf = open(outfile, 'w')

    print (generated_text, file=outf)

    outf.close()

# Codi per contar el nombre de tokens de la entrada (aixi ens assegurem que no ens pasem)
def contar_tokens(model_name, examples_path, sentences_path):
  tokenizer = AutoTokenizer.from_pretrained(model_name)

  # Load the Examples Dataset:
  with open(examples_path, "r", encoding="utf-8") as file:
      examples = file.read().splitlines()

  # First prompt with complete examples:
  full_prompt_examples = f"{full_prompt} . {examples} \n"


  with open(sentences_path, "r", encoding="utf-8") as file:
      lines = file.readlines()

  first_line = True
  for line in lines:
    if first_line:
        first_line = False
        combined_text = f"{full_prompt_examples} . {line} \n Drugs:"
        tokens = tokenizer.tokenize(combined_text)

        numero_de_tokens = len(tokens)

        print(f"Texto original: {combined_text}")
        print(f"Número de tokens: {numero_de_tokens}")
        print("Tokens:", tokens)

def print_times(times, mode):
  print("Duraciones de todas las iteraciones:", times)

  if mode :
    media_tiempos = sum(times[1:]) / (len(times) - 1)
  else:
    media_tiempos = sum(times) / len(times)

  print(f"Media de tiempos de predicción sin contar la primera: {media_tiempos} segundos")

  total_time = sum(times)
  print(f"Tiempo total para la predicción: {total_time} segundos")

def save_txt_drive(ruta_org, ruta_dst):
  # Ruta del archivo en Google Colab
  ruta_colab = ruta_org

  # Ruta de destino en Google Drive
  ruta_drive = ruta_dst

  # Copiar el archivo de Colab a Drive
  shutil.copy(ruta_colab, ruta_drive)

"""## Zero-shot Learning

Cal modificar la part del test que estem tractant i descomentar GPT2 o GPTNeo depenent del model que estiguem utilitzant.
"""

def zero_shot_learning(output_dir): # also used for fine tunned models (without examples)

  model = GPT2LMHeadModel.from_pretrained(output_dir)
  tokenizer = GPT2Tokenizer.from_pretrained(output_dir)

  #model = GPTNeoForCausalLM.from_pretrained(output_dir)
  #tokenizer = AutoTokenizer.from_pretrained(output_dir)

  # Configurar pad_token_id y eos_token_id
  model.config.pad_token_id = tokenizer.eos_token_id
  model.config.eos_token_id = tokenizer.eos_token_id

  # Sentences path:
  sentences_path = "/content/ZS_test_dataset.txt"

  # Open the file in read mode
  with open(sentences_path, "r", encoding="utf-8") as file:
      lines = file.readlines()

  # Text that we want to predict:
  generated_text = ""

  # List with the time of each prediction
  times = []

  # Iterate through the lines
  for line in lines:
      # Obtener la longitud de la línea
      line_length = len(line)

      # Establecer el max_length basado en la longitud de la línea
      max_length = line_length + 50
      combined_text = f"{full_prompt} \n ** \n {line} Response:" # Els ** els utilitzem per marcar que es tracta d'una nova resposta (util per a l'avaluacio)

      input_ids = tokenizer.encode(combined_text, return_tensors="pt")

      # Generate predictions
      attention_mask = input_ids.ne(tokenizer.eos_token_id)

      # Time before starting the prediction
      ini_time = time.time()

      predictions = model.generate(input_ids, max_length=max_length, attention_mask=attention_mask)

      # Time after finishing the prediction
      time_after_prediction = time.time()

      prediction_time = time_after_prediction - ini_time
      times.append(prediction_time)

      # New text dedoced
      new_text = tokenizer.decode(predictions[0], skip_special_tokens=True) + "\n"

      # Decode and append the generated text to the output
      generated_text += new_text


  # Save the generated text, we change the name of the file every time that we change the model for having all of them separated
  save_results(generated_text, "/content/generated_text_FT_few_small_p2.txt")
  save_txt_drive('/content/generated_text_FT_few_small_p2.txt', '/content/gdrive/MyDrive/TFG/generated_text_FT_few_small_p2.txt')
  print_times(times,0)

"""##Few shot learning

Cal descomentar GPT2 o GPTNeo depenent del model que estiguem utilitzant.
"""

def few_shot_learning(output_dir):
  #contar_tokens("gpt2-medium", "/content/FS_test_dataset.txt" , "/content/test.txt")

  model = GPT2LMHeadModel.from_pretrained(output_dir)
  tokenizer = GPT2Tokenizer.from_pretrained(output_dir)

  #model = GPTNeoForCausalLM.from_pretrained(output_dir)
  #tokenizer = AutoTokenizer.from_pretrained(output_dir)

  # Configurar pad_token_id y eos_token_id
  model.config.pad_token_id = tokenizer.eos_token_id

  # Sentences path:
  sentences_path = "/content/test.txt"

  # Examples path:
  examples_path = "/content/FS_test_dataset.txt"

  # Load the Examples Dataset:
  with open(examples_path, "r", encoding="utf-8") as file:
      examples = file.read().splitlines()

  examples_text = "\n".join(examples)

  # First prompt with complete examples:
  full_prompt_examples = f"{full_prompt} \n {examples_text} \n"

  # Open the file in read mode
  with open(sentences_path, "r", encoding="utf-8") as file:
      lines = file.readlines()

  # Text that we want to predict:
  generated_text = ""
  first_line = True

  # List with the time of each prediction
  times = []

  # Iterate through the lines
  for line in lines:
      line_length = len(line)

      max_length = line_length + 50
      # If it is the first line we also combine all the full prompt and examples for the prediction
      if first_line:
        first_line = False
        combined_text = f"{full_prompt_examples} \n ** \n {line} Response:"
        max_length = 1024 # maxim per gpt-2 medium

      # In another case we only combine the prompt
      else:
        combined_text = f"{full_prompt} \n ** \n {line} Response:"

      input_ids = tokenizer.encode(combined_text, return_tensors="pt")

      # Generate predictions
      # max_length it is different depends if we have to pass the prompt with examples or only the prompt and the sentence
      attention_mask = input_ids.ne(tokenizer.eos_token_id)

      # Time before starting the prediction
      ini_time = time.time()
      predictions = model.generate(input_ids, max_length=max_length, attention_mask=attention_mask, num_return_sequences = 1)

      # Time after finishing the prediction
      time_after_prediction = time.time()

      prediction_time = time_after_prediction - ini_time
      times.append(prediction_time)

      # New text dedoced
      new_text = tokenizer.decode(predictions[0], skip_special_tokens=True) + "\n"

      # Decode and append the generated text to the output
      generated_text += new_text


  # Save the generated text
  save_results(generated_text, "/content/generated_text_FS.txt")
  save_txt_drive('/content/generated_text_FS.txt', '/content/gdrive/MyDrive/TFG/generated_text_FS.txt')
  print_times(times,1)

"""##Fine tunning"""

## FINE TUNNING THE MODEL
# Define the GPT-2 model and tokenizer
def fine_tune_gpt2(model_name, train_file, eval_file, output_dir):

  model = GPT2LMHeadModel.from_pretrained(model_name)
  tokenizer = GPT2Tokenizer.from_pretrained(model_name)
  tokenizer.pad_token = tokenizer.eos_token

  # Load training dataset
  train_dataset = TextDataset(
      tokenizer=tokenizer,
      file_path=train_file,
      block_size=128)

  # Load evaluation dataset
  eval_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=eval_file,
        block_size=128)

  # Create data collator for language modeling
  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

  # Define the training arguments. (fine tunning arguments)
  training_args = TrainingArguments(
      output_dir=output_dir,             # Output directory for checkpoints and logs
      overwrite_output_dir=True,         # Overwrite the output directory if it exists
      num_train_epochs=5,                # Total number of training epochs
      per_device_train_batch_size=2,     # Batch size per GPU
      per_device_eval_batch_size=2,
      save_steps=10_000,                 # Save a checkpoint every specified number of steps
      save_total_limit=2,                # Limit the total number of saved checkpoints
      evaluation_strategy="steps",        # Evaluate every `eval_steps`
      eval_steps=500,                     # Evaluate every specified number of steps
  )

  # Define the Trainer with the model and training arguments.
  trainer = Trainer(
      model=model,                        # The GPT-2 model
      args=training_args,
      data_collator=data_collator,        # Use default data collator for text generation
      tokenizer=tokenizer,                # The GPT-2 tokenizer
      train_dataset=train_dataset,        # Your fine-tuning dataset
      eval_dataset=eval_dataset,          # Your evaluation dataset
  )

  # Fine-tune the GPT-2 model.
  trainer.train()

  # Save the fine-tuned model.
  model.save_pretrained(output_dir)
  tokenizer.save_pretrained(output_dir)

  # Define las rutas del directorio en Colab y el directorio en Google Drive
  directorio_en_colab = output_dir
  directorio_en_drive = '/content/gdrive/MyDrive/TFG/gpt2-medium-FT-few-pharma'

  # Copia el directorio completo a Google Drive
  shutil.copytree(directorio_en_colab, directorio_en_drive)

"""## Evaluation"""

def load_predicted_and_expected_data():
  # Examples path:
  expected_path = "/content/full_test_dataset.txt"

  expected_dataset = []

  # Load the Examples Dataset:
  with open(expected_path, "r", encoding="utf-8") as file:
      current_entry = []
      for line in file:
          line = line.strip()
          current_entry.append(line)

          if line == '\n':
              expected_dataset.append(current_entry)
              current_entry = []

  # Add the last line
  if current_entry:
      expected_dataset.append(current_entry)

  print(expected_dataset)

  predicted_path = "/content/gdrive/MyDrive/TFG/generated_text_FT_few_small_p2.txt"

  predicted_dataset = []

  save_line = False
  # Load the Predicted Dataset:
  with open(predicted_path, "r", encoding="utf-8") as file :
      current_entry = []
      for line in file:
          line = line.strip()
          if save_line :
            current_entry.append(line)
          if line == "**" :
            save_line = True
          if line == '\n' :
              predicted_dataset.append(current_entry)
              current_entry = []
          if line.startswith("Response:") and save_line :
            save_line = False

  # Add the last line
  if current_entry :
      predicted_dataset.append(current_entry)

  print(predicted_dataset)

  pre_process_predicted_and_expected_data(predicted_dataset, expected_dataset)

def pre_process_predicted_and_expected_data(predicted_dataset, expected_dataset):
  predicted_dataset = list(chain.from_iterable(predicted_dataset))

  responses_predicted_dataset = predicted_dataset[1::2]

  expected_dataset = list(chain.from_iterable(expected_dataset))

  responses_expected_dataset = expected_dataset[1::2]

  calcule_results(responses_expected_dataset, responses_predicted_dataset)

def calcule_results(responses_expected_dataset, responses_predicted_dataset):
  COR = 0 # correct answers
  POS = 0 # possible answers
  ACT = 0 # actual answers


  for str1, str2 in zip(responses_expected_dataset, responses_predicted_dataset):
    patron = re.compile(r'<(group|drug|drug_n|brand)>(.*?)</\1>')

    coincidencias1 = patron.findall(str1)
    coincidencias2 = patron.findall(str2)

    POS += len(coincidencias1)
    ACT += len(coincidencias2)

    correct = 0

    # predicted
    for i in range(len(coincidencias2)):
        a = coincidencias2[i]

        # expected
        for j in range(len(coincidencias1)):
            b = coincidencias1[j]

            if a == b:
                correct += 1
                coincidencias1.pop(j)
                break

    COR += correct
  print("POS: ")
  print(POS)
  print("ACT: ")
  print(ACT)
  print("COR: ")
  print(COR)
  #evaluadors(COR,ACT,POS) a les subparts no ho executem fins al final

def evaluadors(COR,ACT,POS):
  # Precisio ->
  P = COR/ACT

  # Recall ->
  R = COR/POS

  # F1-score ->
  F1 = (2*P*R) / (P+R)

  print("P: ")
  print(P)
  print("R: ")
  print(R)
  print("F1: ")
  print(F1)

"""#MAIN"""

import sys, torch, shutil
from os import listdir,system
import re
import time

import nltk
nltk.download('punkt')

from xml.dom.minidom import parse
from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, GPTNeoForSequenceClassification, GPTNeoForCausalLM
from transformers import Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling #for fine tuning
from torch.utils.data import Dataset
from itertools import chain, islice

pre_processing_FT()

fine_tune_gpt2("gpt2-medium", "/content/train_dataset.txt", "/content/devel_dataset.txt", "./gpt2-medium-pharma-few")

pre_processing_without_fine_tunning()

# Define the prompt text and the list of unseen class names
full_prompt = "For each of the following sentences, mark in XML which are the mentioned drugs and their type(drug, group, brand, drug not usable on humans)."

## ZSL & FT
zero_shot_learning('/content/gdrive/MyDrive/TFG/gpt2-small-FT-few-pharma')

## FSL
few_shot_learning('/content/gdrive/MyDrive/TFG/gpt2-pharma')

load_predicted_and_expected_data()